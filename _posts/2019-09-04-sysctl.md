---
title: sysctl参数解释
date: 2019-09-04 02:00:00
categories:
- Linux
tags:
- 运维
description: sysctl -a 一些参数的解释
---

[内核参数说明](https://www.cnblogs.com/tolimit/p/5065761.html)

> fs.aio-max-nr 和 fs.aio-nr

* `aio-max-nr`是允许异步IO并发请求的最大数量。
* `aio-nr`是当前系统的异步IO请求的数量。Oracle支持异步IO配置。PostgreSQL, Greenplum均未使用io_setup创建aio contexts，无需设置。

> fs.file-max 和 fs.file-nr 和 fs.nr_open

* `file-max`是系统内核能够分配的的文件句柄最大数量。
* `file-nr`是三个值：已经分配文件句柄数量，已经分配但是未使用的文件句柄数量，系统能分配的最大文件句柄数量即`file-max`。
* `nr_open`是 一个进程能够打开的最大文件数。

> kernel.sem = 250 32000 32 128

&emsp;&emsp;`ipcs -s -l`可以看到解释。分别是：
```
        250       SEMMSL    max semaphores per array     信号集容纳最大信号数量  
        32000     SEMMNS    max semaphores system wide   所有信号的最大数量
        32        SEMOPM    max ops per semop call       调用单个信号集中最大信号数量
        128       SEMMNI    max number of arrays         信号集的最大值
```
PostgreSQL每16个进程一组, 每组需要17个信号量，每个进程需要一个信号量，然后每16个需要一个额外的信号量“magic number”。
所以`SEMMSL >= 17`是必须的。128个信号集合，能够支持128*16=2048个进程。大多数是连接进程。

> kernel.shmall 和 kernel.shmmax 和 kernel.shmmni

* `shmmax`单个共享内存段最大尺寸(postgres推荐主机内存的一半，单位字节)。
* `shmall`所有共享内存段加起来最大PAGE数(postgres推荐主机内存的80%，单位PAGE)。
* `shmmni`一共允许创建共享内存段数量(每个数据库启动需要2个共享内存段，将来允许动态创建共享内存段，可能需求量更大)。

+ PostgreSQL 9.2以及以前的版本，数据库启动时，对共享内存段的内存需求非常大，需要考虑以下几点。后期的版本同样适用:
```
        Connections:                (1800 + 270 * max_locks_per_transaction) * max_connections 
        Autovacuum workers:         (1800 + 270 * max_locks_per_transaction) * autovacuum_max_workers 
        Prepared transactions:      (770 + 270 * max_locks_per_transaction) * max_prepared_transactions 
        Shared disk buffers:        (block_size + 208) * shared_buffers 
        WAL buffers:                (wal_block_size + 8) * wal_buffers 
        Fixed space requirements:   770 kB
```

> net.core.rmem_default 和 net.core.rmem_max

&emsp;&emsp;接收套接字缓冲区大小的默认值和最大值(以字节为单位)。

> net.core.wmem_default 和 net.core.wmem_max

&emsp;&emsp;发送套接字缓冲区大小的默认值和最大值(以字节为单位)。

> net.core.optmem_max

&emsp;&emsp;表示每个套接字所允许的最大缓冲区的大小。

> net.core.netdev_max_backlog

&emsp;&emsp;当网卡接收数据包的速度大于内核处理的速度时，会在INPUT侧有一个队列保存这些数据包。这个参数表示该队列的最大值。INPUT链表越长，处理耗费越大，如果用了iptables管理的话，需要加大这个值。

> net.core.somaxconn

&emsp;&emsp;用来限制每一个端口监听(LISTEN)队列的最大长度，超过这个数量就会导致链接超时或者触发重传机制。

> net.ipv4.tcp_max_syn_backlog

&emsp;&emsp;表示那些尚未收到客户端确认信息的连接（SYN消息）队列的长度,增大这个值,可以容纳更多等待连接的网络连接数。

> net.ipv4.tcp_keepalive_time

&emsp;&emsp;表示当keepalive启用时，连接每次空闲多久，TCP发送keepalive消息。默认是2小时（7200s），若将其设置得小一些，可以更快地清理无效的连接。

> net.ipv4.tcp_keepalive_intvl

&emsp;&emsp;keepalive探测消息未获得响应时，间隔多少秒，重发该消息。系统默认75秒。

> net.ipv4.tcp_keepalive_probes

&emsp;&emsp;发送多少次TCP的keepalive探测包无响应后，认定连接失效。系统默认值是9。这个值乘以`tcp_keepalive_intvl`之后决定了，一个连接发送了keepalive探测包之后可以有多少时间没有回应。

> net.ipv4.tcp_mem = 1539615 2052821 3079230

&emsp;&emsp;确定TCP栈应该如何反映内存使用，每个值的单位都是内存页（通常是4KB）。
```
        第一个值    内存使用的下限。
        第二个值    内存压力模式开始对缓冲区使用的上限。
        第三个值    内存使用的上限。在这个层次上可以将报文丢弃，从而减少对内存的使用。
```

> net.ipv4.tcp_max_orphans

&emsp;&emsp;表示系统中最多有多少TCP套接字不被关联到任何一个用户文件句柄上。如果超过这里设置的数字，连接就会复位并输出警告信息。这个限制仅仅是为了防止简单的DoS攻击。此值不能太小。

> net.ipv4.tcp_max_tw_buckets = 10000

&emsp;&emsp;表示系统同时保持TIME_WAIT套接字的最大数量。如果超过此数，TIME_WAIT套接字会被立刻清除并且打印警告信息。之所以要设定这个限制，纯粹为了抵御那些简单的DoS攻击，不过，过多的TIME_WAIT套接字也会消耗服务器资源，甚至死机。

> net.ipv4.tcp_tw_recycle = 1

&emsp;&emsp;表示开启TCP连接中TIME_WAIT套接字的快速回收，默认为0，表示关闭。

> net.ipv4.tcp_tw_reuse = 1

&emsp;&emsp;表示允许重用TIME_WAIT状态的套接字用于新的TCP连接,默认为0，表示关闭。

> net.ipv4.tcp_timestamps = 1

&emsp;&emsp;`tcp_timestamps`是`TCP`协议中的一个扩展项，通过时间戳的方式来检测过来的包以防止`PAWS`(Protect Against Wrapped Sequence numbers)，可以提高`TCP`的性能。`net.ipv4.tcp_tw_recycle`和`net.ipv4.tcp_timestamps`不建议同时开启。

> net.ipv4.tcp_fin_timeout = 30

&emsp;&emsp;减少处于`FIN-WAIT-2`连接状态的时间，单位秒，使系统可以处理更多的连接。加快僵尸连接回收速度。

> net.ipv4.tcp_syncookies = 1

&emsp;&emsp;当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭。防止syn flood攻击。

> net.ipv4.tcp_synack_retries

&emsp;&emsp;表示系统允许SYN连接的重试次数。为了打开对端的连接，内核需要发送一个SYN并附带一个回应前面一个SYN的ACK包。也就是所谓三次握手中的第二次握手。这个设置决定了内核放弃连接之前发送SYN+ACK包的数量。

> net.ipv4.tcp_syn_retries

&emsp;&emsp;表示在内核放弃建立连接之前发送SYN包的数量。

> net.ipv4.ip_local_port_range = 1024 65500

&emsp;&emsp;表示允许系统打开的端口范围。

> net.ipv4.tcp_rmem = 8192 87380 16777216

&emsp;&emsp;为自动调优定义socket使用的内存。
```
        第一个值    为socket接收缓冲区分配的最少字节数；
        第二个值    默认值（该值会被net.core.rmem_default覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；
        第三个值    接收缓冲区空间的最大字节数（该值会被net.core.rmem_max覆盖）。
```

> net.ipv4.tcp_wmem = 8192 87380 16777216

&emsp;&emsp;为自动调优定义socket使用的内存。
```
        第一个值    为socket发送缓冲区分配的最少字节数；
        第二个值    默认值（该值会被net.core.wmem_default覆盖），缓冲区在系统负载不重的情况下可以增长到这个值；
        第三个值    发送缓冲区空间的最大字节数（该值会被net.core.wmem_max覆盖）。
```

> net.nf_conntrack_max 和 net.netfilter.nf_conntrack_max

&emsp;&emsp;Size of connection tracking table。CentOS 6可以调整这个参数。

> vm.dirty_background_ratio = 10

&emsp;&emsp;表示当脏页比例达到了内存的10%，系统触发background flush线程刷脏页

> vm.dirty_background_bytes = 102400000

&emsp;&emsp;表示当脏页数达到了100MB，系统触发background flush线程刷脏页

> vm.dirty_writeback_centisecs = 50

&emsp;&emsp;background flush线程的唤醒间隔(单位：百分之一秒)

> vm.dirty_expire_centisecs = 6000

&emsp;&emsp;background flush线程将存活时间超过该值的脏页刷盘（类似LRU）(单位：百分之一秒)

> vm.dirty_ratio = 20

&emsp;&emsp;表示当脏页比例达到20%，用户进程在调用write时，会触发flush磁盘的操作。

> vm.dirty_bytes = 204800000

&emsp;&emsp;表示当脏页数达到了200MB，用户进程在调用write时，会触发flush磁盘的操作。

> vm.extra_free_kbytes

&emsp;&emsp;目标是让后台进程回收内存更积极，比用户进程提早多少kbytes回收，因此用户进程可以快速分配内存。CentOS 6的参数。

> vm.min_free_kbytes 和 lowmem_reserve_ratio

* `min_free_kbytes`：系统所保留空闲内存的最低限。防止在高负载时系统无响应，减少内存分配死锁概率。
* 除了`min_free_kbytes`会在每个zone上预留一部分内存外，`lowmem_reserve_ratio`是在各个zone之间进行一定的防卫预留，主要是防止高端zone在没内存的情况下过度使用低端zone的内存资源。 

&emsp;&emsp;[相关介绍](https://blog.csdn.net/joyeu/article/details/20063429)

> vm.extfrag_threshold

&emsp;&emsp;这个参数是一个0 ~ 1000的整数。如果出现内存不够用的情况，Linux会为当前系统的内存碎片情况打一个分，如果超过了extfrag_threshold这个值，kswapd就会触发memory compaction。
所以，这个值设置接近1000，说明系统在内存碎片的处理倾向于把旧的页换出，以符合申请的需要；而设置接近0，表示系统在内存碎片的处理倾向于做memory compaction。
&emsp;&emsp;[如何控制Linux清理cache机制?](https://www.zhihu.com/question/59053036/answer/171176545)

> vm.panic_on_oom 和 vm.oom_kill_allocating_task 和 vm.oom_dump_tasks

&emsp;&emsp;当物理内存和交换空间都被用完时，如果还有进程来申请内存，内核将触发OOM killer，其行为如下：
* 检查文件/proc/sys/vm/panic_on_oom，如果里面的值为2，那么系统一定会触发panic。
* 如果/proc/sys/vm/panic_on_oom的值为1，那么系统有可能触发panic（见后面的介绍）。
* 如果/proc/sys/vm/panic_on_oom的值为0，或者上一步没有触发panic，那么内核继续检查文件/proc/sys/vm/oom_kill_allocating_task。
* 如果/proc/sys/vm/oom_kill_allocating_task为1，那么内核将kill掉当前申请内存的进程。
* 如果/proc/sys/vm/oom_kill_allocating_task为0，内核将检查每个进程的分数，分数最高的进程将被kill掉（见后面介绍）。
* 进程被kill掉之后，如果/proc/sys/vm/oom_dump_tasks为1，且系统的rlimit中设置了core文件大小，将会由/proc/sys/kernel/core_pattern里面指定的程序生成core dump文件。这个文件里将包含pid, uid, tgid, vm size, rss, nr_ptes, nr_pmds, swapents, oom_score_adjscore, name等内容。拿到这个core文件之后，可以做一些分析，看为什么这个进程被选中kill掉。

> vm.would_have_oomkilled

&emsp;&emsp;设置为1，也不会真正的去杀死进程

> vm.memory_failure_early_kill = 0

&emsp;&emsp;控制在某个内核无法处理的内存错误发生的时候，如何去杀掉这个进程。当这些页有swap镜像的时候，内核会很好的处理这个错误，不会影响任何应用程序，但是如果没有的话，内核会把进程杀掉，避免内存错误的扩大。
&emsp;&emsp;为1的时候，在发现内存错误的时候，就会把所有拥有内存错误的进程都杀掉。
&emsp;&emsp;为0的时候，只是对这部分页进行unmap，然后把第一个试图进入这个页的进程杀掉

> vm.memory_failure_recovery = 1

&emsp;&emsp;是否开启内存错误恢复机制。为1的时候，开启。为0的时候，一旦出现内存错误，就panic。

> vm.overcommit_memory = 0

&emsp;&emsp;vm.overcommit_memory文件指定了内核针对内存分配的策略，其值可以是：
``` 
        0：(默认)表示内核将检查是否有足够的可用内存供应用进程使用。启发式的overcommitting handle,会尽量减少swap的使用,root可以分配比一般用户略多的内存。
        1：表示内核允许分配所有的物理内存，而不管当前的内存状态如何，允许超过CommitLimit，直至内存用完为止。在数据库服务器上不建议设置为1，从而尽量避免使用swap。
        2：表示不允许超过CommitLimit值。
```
&emsp;&emsp;执行`grep -i commit /proc/meminfo`看到`CommitLimit`和`Committed_As`参数。
* `CommitLimit`是一个内存分配上限， CommitLimit = 物理内存 * overcommit_ratio(默认50，即50%) + swap大小
* `Committed_As`是已经分配的内存大小。

> vm.overcommit_ratio = 50

&emsp;&emsp;这个参数值只有在`vm.overcommit_memory=2`的情况下，内存可分配的百分比

> vm.mmap_min_addr = 4096

&emsp;&emsp;指定用户进程通过mmap可使用的最小虚拟内存地址，以避免其在低地址空间产生映射导致安全问题；如果非0，则不允许mmap到NULL页，而此功能可在出现NULL指针时调试Kernel；mmap用于将文件映射至内存。该设置意味着禁止用户进程访问low 4k地址空间。

> vm.min_unmapped_ratio = 1

&emsp;&emsp;只有在当前内存域中处于`zone_reclaim_mode`允许回收状态的内存页所占的百分比超过min_unmapped_ratio时，内存域才会执行回收操作。

> vm.percpu_pagelist_fraction = 0

&emsp;&emsp;每个CPU能从每个zone所能分配到的pages的最大值(单位每个zone的1/X)，0为不限制

> vm.max_map_count = 65530

&emsp;&emsp;定义了一个进程能拥有的最多的内存区域。

> vm.legacy_va_layout = 0

&emsp;&emsp;非0的时候，就禁止32位的mmap映射方式，采用legacy（2.4）的映射方式。

> vm.vfs_cache_pressure = 100

&emsp;&emsp;该项表示内核回收用于directory和inode cache内存的倾向：
&emsp;&emsp;&emsp;&emsp;缺省值100表示内核将根据pagecache和swapcache，把directory和inode cache保持在一个合理的百分比。
&emsp;&emsp;&emsp;&emsp;降低该值低于100，将导致内核倾向于保留directory和inode cache。
&emsp;&emsp;&emsp;&emsp;增加该值超过100，将导致内核倾向于回收directory和inode cache。
&emsp;&emsp;网上文章建议`sysctl -w vm.vfs_cache_pressure=200`其实一般情况下不需要调整，只有在极端场景下才建议进行调整，只有此时，才有必要进行调优，这也是调优的意义所在。

> vm.nr_hugepages

&emsp;&emsp;huge page页数量，大内存机器建议设置它。64G以上。

> vm.nr_overcommit_hugepages = 0

&emsp;&emsp;保留于紧急使用的大页数，系统可分配最大大页数= `nr_hugepages` + `nr_overcommit_hugepage`

> vm.hugetlb_shm_group = 0

&emsp;&emsp;哪些用户组可以在hugepages上创建共享内存

> vm.nr_hugepages_mempolicy = 0

&emsp;&emsp;与`nr_hugepages`类似，但只用于numa架构，配合numactl调整每个node的大页数量

> vm.hugepages_treat_as_movable = 0

&emsp;&emsp;用来控制是否可以从ZONE_MOVABLE内存域中分配大页面。如果设置为非零，大页面可以从ZONE_MOVABLE内存域分配。ZONE_MOVABLE内存域只有在指定了kernelcore启动参数的情况下才会创建，如果没有指定kernelcore启动参数，hugepages_treat_as_movable参数则没有效果。

> vm.swappiness = 0

&emsp;&emsp;表示尽量使用内存，减少使用磁盘swap交换分区。这个值越高，就越建议内核使用swap。

> vm.zone_reclaim_mode = 0

&emsp;&emsp;不使用NUMA。






